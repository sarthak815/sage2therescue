{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bda18f-1757-4d7a-904d-70189a6c7fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.22.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.8 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.22.1\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (1.22.1)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (635 kB)\n",
      "\u001b[K     |████████████████████████████████| 635 kB 55.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.9 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (59.4.0)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 13.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 55.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 103.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 100.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[K     |████████████████████████████████| 452 kB 90.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 114.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.6)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 17.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 127.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 21.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 108.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.11-py3-none-any.whl (39 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 21.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions, murmurhash, cymem, click, catalogue, wasabi, urllib3, typer, srsly, smart-open, pydantic, preshed, idna, charset-normalizer, certifi, blis, tqdm, thinc, spacy-loggers, spacy-legacy, requests, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 certifi-2021.10.8 charset-normalizer-2.0.11 click-8.0.3 cymem-2.0.6 idna-3.3 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 requests-2.27.1 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 tqdm-4.62.3 typer-0.4.0 typing-extensions-4.0.1 urllib3-1.26.8 wasabi-0.9.0\n",
      "ERROR: unknown command \"intall\" - maybe you meant \"install\"\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install spacy\n",
    "!pip intall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1b1622-e137-48e4-a84b-5aec32c5fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (1.22.1)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 96.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.0 pytz-2021.3\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.4 MB 3.0 MB/s eta 0:00:01     |███████████████████             | 15.8 MB 3.0 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.8 MB 77.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.22.1)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 103.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=ecb0d0bf8dc3e06c28957277836805e586cd5b670e470b69e0a4fac7baff0f1e\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.1.18-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "\u001b[K     |████████████████████████████████| 763 kB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.6.7 regex-2022.1.18\n",
      "Requirement already satisfied: spacy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (1.22.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (59.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install nltk\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667f4024-5c2b-4952-92d3-75a9b4896109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/studio-lab-user/.conda/envs/default\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _openmp_mutex-4.5          |           1_llvm           5 KB  conda-forge\n",
      "    future-0.18.2              |   py39hf3d152e_4         715 KB  conda-forge\n",
      "    libblas-3.9.0              |   13_linux64_mkl          13 KB  conda-forge\n",
      "    libcblas-3.9.0             |   13_linux64_mkl          12 KB  conda-forge\n",
      "    liblapack-3.9.0            |   13_linux64_mkl          12 KB  conda-forge\n",
      "    libprotobuf-3.19.4         |       h780b84a_0         2.6 MB  conda-forge\n",
      "    llvm-openmp-12.0.1         |       h4bd325d_1         2.8 MB  conda-forge\n",
      "    mkl-2022.0.1               |     h8d4b97c_803       199.3 MB  conda-forge\n",
      "    ninja-1.10.2               |       h4bd325d_1         2.4 MB  conda-forge\n",
      "    numpy-1.22.1               |   py39h91f2184_0        19.3 MB  conda-forge\n",
      "    pytorch-1.10.1             |cpu_py39h5e9ed0b_0        54.3 MB  conda-forge\n",
      "    sleef-3.5.1                |       h9b69904_2         1.5 MB  conda-forge\n",
      "    tbb-2021.5.0               |       h4bd325d_0         2.0 MB  conda-forge\n",
      "    typing_extensions-4.0.1    |     pyha770c72_0          26 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       285.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  future             conda-forge/linux-64::future-0.18.2-py39hf3d152e_4\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-13_linux64_mkl\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-13_linux64_mkl\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-13_linux64_mkl\n",
      "  libprotobuf        conda-forge/linux-64::libprotobuf-3.19.4-h780b84a_0\n",
      "  llvm-openmp        conda-forge/linux-64::llvm-openmp-12.0.1-h4bd325d_1\n",
      "  mkl                conda-forge/linux-64::mkl-2022.0.1-h8d4b97c_803\n",
      "  ninja              conda-forge/linux-64::ninja-1.10.2-h4bd325d_1\n",
      "  numpy              conda-forge/linux-64::numpy-1.22.1-py39h91f2184_0\n",
      "  pytorch            conda-forge/linux-64::pytorch-1.10.1-cpu_py39h5e9ed0b_0\n",
      "  sleef              conda-forge/linux-64::sleef-3.5.1-h9b69904_2\n",
      "  tbb                conda-forge/linux-64::tbb-2021.5.0-h4bd325d_0\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-4.0.1-pyha770c72_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  _openmp_mutex                                   4.5-1_gnu --> 4.5-1_llvm\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pytorch-1.10.1       | 54.3 MB   | ##################################### | 100% \n",
      "mkl-2022.0.1         | 199.3 MB  | ##################################### | 100% \n",
      "typing_extensions-4. | 26 KB     | ##################################### | 100% \n",
      "sleef-3.5.1          | 1.5 MB    | ##################################### | 100% \n",
      "libblas-3.9.0        | 13 KB     | ##################################### | 100% \n",
      "libcblas-3.9.0       | 12 KB     | ##################################### | 100% \n",
      "numpy-1.22.1         | 19.3 MB   | ##################################### | 100% \n",
      "ninja-1.10.2         | 2.4 MB    | ##################################### | 100% \n",
      "_openmp_mutex-4.5    | 5 KB      | ##################################### | 100% \n",
      "liblapack-3.9.0      | 12 KB     | ##################################### | 100% \n",
      "llvm-openmp-12.0.1   | 2.8 MB    | ##################################### | 100% \n",
      "tbb-2021.5.0         | 2.0 MB    | ##################################### | 100% \n",
      "future-0.18.2        | 715 KB    | ##################################### | 100% \n",
      "libprotobuf-3.19.4   | 2.6 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448cd558-3d1b-441f-af43-c66dd26d1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import string\n",
    "import statistics\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e0728b-0275-4a88-8708-56eae3160a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:    \n",
    "    EMB_SIZE = 300    \n",
    "    NUM_FOLDS = 5\n",
    "    NUM_EPOCHS = 20\n",
    "    NUM_CV_RUN = 1\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "\n",
    "DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105f6961-d6f8-4898-9732-4070c797f12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in train.csv = 18983\n",
      "Rows in test.csv = 3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location  \\\n",
       "0  1.0     NaN      NaN   \n",
       "1  4.0     NaN      NaN   \n",
       "2  5.0     NaN      NaN   \n",
       "3  6.0     NaN      NaN   \n",
       "4  7.0     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv',encoding= 'unicode_escape')\n",
    "df_test = pd.read_csv('test.csv',encoding= 'unicode_escape')\n",
    "print(f\"Rows in train.csv = {len(df_train)}\")\n",
    "print(f\"Rows in test.csv = {len(df_test)}\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1216785-f06f-4555-a4bf-10f0f18fba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive training examples = 5385\n",
      "No. of negative training examples = 13598\n",
      "No. of unique keywords = 222\n",
      "No of train examples with keyword not null = 18922\n"
     ]
    }
   ],
   "source": [
    "df_train_pos = df_train[df_train.target == 1]\n",
    "df_train_neg = df_train[df_train.target == 0]\n",
    "print(f\"No. of positive training examples = {len(df_train_pos)}\")\n",
    "print(f\"No. of negative training examples = {len(df_train_neg)}\")\n",
    "train_keywords_unique = df_train.keyword.unique()\n",
    "print(f\"No. of unique keywords = {len(train_keywords_unique)}\")\n",
    "df_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\n",
    "print(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339b895-7508-45a2-bb34-3ed1549cf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet_vectors = None\n",
    "test_tweet_vectors = None\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "with nlp.disable_pipes():\n",
    "    train_tweet_vectors = np.array([nlp(row.text).vector for id, row in df_train.iterrows()])\n",
    "    test_tweet_vectors = np.array([nlp(row.text).vector for id, row in df_test.iterrows()])\n",
    "print(train_tweet_vectors.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1f3cd-6a58-4e30-8878-a906d2eb073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = df_train[\"target\"]\n",
    "vec_mean = train_tweet_vectors.mean(axis=0)\n",
    "vec_std = train_tweet_vectors.std(axis=0)\n",
    "print(vec_mean.shape, vec_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e60f1b-99e9-4c51-a78d-1b29c14c2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=Config.NUM_FOLDS, X=train_tweet_vectors, y=train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086391a3-2502-4668-b3a4-81f3ce15cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation data for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_data(fold, kfolds, X, y):\n",
    "    fold += 1                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]    \n",
    "    return train_X, train_y, val_X, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193cf90-8890-4440-a104-026132623c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def run_training(train_X, train_y, val_X, val_y, params):\n",
    "    # Create the SVC model\n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    # normalize the train and validation data \n",
    "    scaler = StandardScaler()\n",
    "    train_X_scaled = scaler.fit_transform(train_X.astype(np.float32))\n",
    "    val_X_scaled = scaler.fit_transform(val_X.astype(np.float32))\n",
    "    model.fit(train_X_scaled, train_y.ravel())\n",
    "    val_y_pred = model.predict(val_X_scaled)\n",
    "    val_score = None\n",
    "    if Config.MODEL_EVAL_METRIC == MODEL_EVAL_METRIC.accuracy:\n",
    "        val_score = accuracy_score(val_y, val_y_pred)\n",
    "    elif Config.MODEL_EVAL_METRIC == MODEL_EVAL_METRIC.f1_score:\n",
    "        val_score = f1_score(val_y, val_y_pred)\n",
    "    return round(val_score, 4), model\n",
    "import tqdm\n",
    "\n",
    "def cv_run(run, train_tweet_vectors, train_targets):\n",
    "    fold_metrics_model = []\n",
    "    params = {\"C\": 1.0, \"kernel\": \"rbf\"}\n",
    "    for fold in tqdm.tqdm(range(Config.NUM_FOLDS)):        \n",
    "        train_X, train_y, val_X, val_y = get_fold_data(fold, k_folds, train_tweet_vectors, train_targets)    \n",
    "        fold_val_metric, fold_model = run_training(train_X, train_y, val_X, val_y, params)\n",
    "        fold_metrics_model.append((fold_val_metric, fold_model))\n",
    "    fold_metrics = [item[0] for item in fold_metrics_model]\n",
    "    cv_metric_mean = statistics.mean(fold_metrics)\n",
    "    cv_metric_std = statistics.stdev(fold_metrics)\n",
    "    print(f\"For cv run {run} fold_metrics = {fold_metrics}\")\n",
    "    print(f\"mean val metric across folds = {cv_metric_mean}, val metric stdev across folds = {cv_metric_std}\")\n",
    "    return fold_metrics_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652cc548-74ec-4f3b-9a8c-3702c29bb472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:28<01:52, 28.06s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:57<01:26, 28.78s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:27<00:58, 29.21s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:55<00:28, 28.92s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:25<00:00, 29.07s/it]\u001b[A\n",
      "100%|██████████| 1/1 [02:25<00:00, 145.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cv run 1 fold_metrics = [0.8594, 0.8512, 0.848, 0.8656, 0.8559]\n",
      "mean val metric across folds = 0.85602, val metric stdev across folds = 0.006905939472656886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_fold_metrics_model = []\n",
    "train_tweet_vectors = train_tweet_vectors - vec_mean\n",
    "for run in tqdm.tqdm(range(Config.NUM_CV_RUN)):\n",
    "    fold_metrics_model = cv_run(run+1, train_tweet_vectors, train_targets)    \n",
    "    run_fold_metrics_model.append(fold_metrics_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53b13883-1bc2-4acd-b68c-032c86734516",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics_model = run_fold_metrics_model[0]\n",
    "fold_metrics_model_sorted = sorted(fold_metrics_model, key=lambda x:x[0], reverse=True)    \n",
    "best_cv_model = fold_metrics_model_sorted[0][1]\n",
    "\n",
    "def test_predictions(model, test_tweet_vectors):\n",
    "    scaler = StandardScaler()\n",
    "    test_tweet_vectors_scaled = scaler.fit_transform(test_tweet_vectors.astype(np.float32))\n",
    "    predictions = model.predict(test_tweet_vectors_scaled)\n",
    "    print(f\"Completed prediction for {len(predictions)} test rows\")\n",
    "    df_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "    df_submission['target']= predictions\n",
    "    df_submission.to_csv('submission_svc.csv',index=False)\n",
    "    \n",
    "#test_predictions(best_cv_model, test_tweet_vectors)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882d6b4f-0ec3-4fa6-abc0-86a78afb5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed prediction for 3263 test rows\n"
     ]
    }
   ],
   "source": [
    "params = {\"C\": 1.0, \"kernel\": \"rbf\"}\n",
    "full_model = SVC(gamma='scale', C=params[\"C\"], kernel=params[\"kernel\"])\n",
    "# normalize the train and validation data \n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_tweet_vectors.astype(np.float32))\n",
    "full_model.fit(train_X_scaled, train_targets.ravel())\n",
    "test_predictions(full_model, test_tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f1914-28a5-440c-b039-2cdcbd324e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
